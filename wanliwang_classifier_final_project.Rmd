---
title: "classifier_final_project"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE,warning=FALSE,echo=FALSE}
library(gplots)
library(class)
library(RColorBrewer)
library(e1071)
library(caret)
library(randomForest)
library(survival)
library(survminer)
library(ggplot2)
```


### Load the datasets
```{r}
rna_TCGA=read.table("~/Desktop/Classifier_Final_Project_2020/RNASeqData.BRCA.TCGA.txt",sep=" ")
dim(rna_TCGA)
# 20466 1208
# genes samples
pam50_class_micro=read.table("~/Desktop/Classifier_Final_Project_2020/PAM50.class.microarray.txt",sep=" ")
pam50_class_micro[,1]=as.character(pam50_class_micro[,1])
dim(pam50_class_micro)
# 626   1
# rownames are samples, 1 column is the subtype of breast cancers
# 514 cancer samples, 112 normal controls
pam50_genes=read.table("~/Desktop/Classifier_Final_Project_2020/PAM50.genes.txt",sep=" ")
pam50_genes[,1]=as.character(pam50_genes[,1])
dim(pam50_genes)
#50 1,  50 genes
survival_data=read.table("~/Desktop/Classifier_Final_Project_2020/SurvivalData.txt",sep=" ")
dim(survival_data)
# 1041    3

```

### Normalize training set
```{r}
## delete the no expression genes
rna_TCGA_exp=rna_TCGA[-which(rowSums(rna_TCGA)==0),]
dim(rna_TCGA_exp)
# 20187  1208

## logrithm
rna_tem=log2(1+rna_TCGA_exp)

## median centering
median_center=function(x){
  x=as.numeric(x)
  list=x-median(x)
  return(list)
}

rna_norm=apply(rna_tem,1,median_center)
rna_norm=t(rna_norm)

dim(rna_norm)
# 20187  1208
rownames(rna_norm)=rownames(rna_tem)
colnames(rna_norm)=colnames(rna_tem)
```

```{r}
pam50.class.colors = rep("red",nrow(pam50_class_micro))
pam50.class.colors[pam50_class_micro[,1]=="Luminal A"] = "darkgreen"
pam50.class.colors[pam50_class_micro[,1]=="Luminal B"] = "yellow"
pam50.class.colors[pam50_class_micro[,1]=="HER2-enriched"] = "blue"
pam50.class.colors[pam50_class_micro[,1]=="Normal-like"] = "purple"
```

### Source function
```{r}
perform_t_tests_all_rows = function(dataGroup1,dataGroup2){
  nGroup1 = ncol(dataGroup1)
  nGroup2 = ncol(dataGroup2)
  dataAll = cbind(dataGroup1,dataGroup2)
  tTestWithErrorHandling = function(x){
    testResult = try(t.test(x[1:nGroup1],x[(nGroup1+1):(nGroup1+nGroup2)]),silent=TRUE);
    if(is.character(testResult)){
      warning(testResult)
      c(NA,NA,NA)
    }else{
      c(testResult$p.value,testResult$estimate)
    }
  }
  results = matrix(unlist(apply(dataAll,1,tTestWithErrorHandling)),ncol=3,byrow=TRUE)
  colnames(results) = c("P.value","Mean.group.1","Mean.group.2")
  rownames(results) = rownames(dataGroup1)
  results
}

perform_t_tests_all_classes_one_vs_rest = function(dataMatrix,classVector){
  if(ncol(dataMatrix)!=length(classVector)){
    stop("Number of columns of data matrix must be equal to the length of the class vector")
  }
  possibleClasses = unique(classVector)
  nClasses = length(possibleClasses)
  
  allPvalues = matrix(NA,nrow=nrow(dataMatrix),ncol=nClasses)
  allDiffMeans = matrix(NA,nrow=nrow(dataMatrix),ncol=nClasses)
  colnames(allPvalues) = possibleClasses
  rownames(allPvalues) = rownames(dataMatrix)
  colnames(allDiffMeans) = possibleClasses
  rownames(allDiffMeans) = rownames(dataMatrix)
  
  for(i in 1:nClasses){
    class = possibleClasses[i]
    resultTest = perform_t_tests_all_rows(dataMatrix[,classVector==class],dataMatrix[,classVector!=class])
    allPvalues[,i] = resultTest[,1]
    allDiffMeans[,i] = resultTest[,2]-resultTest[,3]
  }
  result = list(allPvalues,allDiffMeans)
  names(result) = c("P.Values","Difference.Between.Means")
  return(result)
}

perform_t_tests_all_classes_each_pair = function(dataMatrix,classVector){
  if(ncol(dataMatrix)!=length(classVector)){
    stop("Number of columns of data matrix must be equal to the length of the class vector")
  }
  possibleClasses = unique(classVector)
  nClasses = length(possibleClasses)
  
  allPValues = NULL
  allDiffMeans = NULL
  names = NULL 
  for(i in 1:(nClasses-1)){
    for(j in (i+1):nClasses){
      class1 = possibleClasses[i]
      class2 = possibleClasses[j]
      names = c(names,paste(class1,class2,sep="."))
      result = perform_t_tests_all_rows(dataMatrix[,classVector==class1],dataMatrix[,classVector==class2])
      allPValues = cbind(allPValues,result[,1])
      allDiffMeans = cbind(allDiffMeans,result[,2]/result[,3])
    }
  }
  colnames(allPValues) = names
  colnames(allDiffMeans) = names
  result = list(allPValues,allDiffMeans)
  names(result) = c("P.Values","Difference.Between.Means")
  return(result)
}


```

### Feature selection
```{r}
rownames(pam50_class_micro)=gsub("-",".",rownames(pam50_class_micro))
rna_known=rna_norm[,which(colnames(rna_norm)%in%rownames(pam50_class_micro))]
rna_known=rna_known[,rownames(pam50_class_micro)]
dim(rna_known)
# 20187   626
rna_unknown=rna_norm[,-which(colnames(rna_norm)%in%rownames(pam50_class_micro))]
dim(rna_unknown)
# 20187   582

### Do feature selection on rna_known
tTestResultOneVsRest = perform_t_tests_all_classes_one_vs_rest(dataMatrix = rna_known,classVector= pam50_class_micro[,1])

tTestResultEachPair = perform_t_tests_all_classes_each_pair(dataMatrix = rna_known, classVector = pam50_class_micro[,1])

## Go through the results of the t-tests and select genes that
## have most significant differences between classes (shown in class)

chosenProbes.one = c()
for(i in 1:5){
  sigProbes = row.names(tTestResultOneVsRest$P.Values[tTestResultOneVsRest$P.Values[,i] < 1e-8,])
  sigDiff = tTestResultOneVsRest$Difference.Between.Means[which(row.names(tTestResultOneVsRest$P.Values) %in% sigProbes), i]
  highDiffProbes = names(tail(sort(sigDiff),10))
  lowDiffProbes = names(head(sort(sigDiff),10))
  chosenProbes.one = c(chosenProbes.one, highDiffProbes, lowDiffProbes)
}

id.one <- duplicated(chosenProbes.one) | duplicated(chosenProbes.one, fromLast = TRUE) 
chosenProbes.one.dup = chosenProbes.one[!id.one] 

```

#### onevsRest 
```{r}
heatmap.2(as.matrix(rna_known[chosenProbes.one.dup,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          cexRow = 0.4,
          main = "heatmap.onevsRest")

ggsave("~/Desktop/figureone.png",width = 8,height = 6)
```


```{r}
####For all 
chosenProbes.pair = c()
for(j in 1:10){
  sigProbes.pair = row.names(tTestResultEachPair$P.Values[tTestResultEachPair$P.Values[,j] < 1e-8,])
  sigDiff.pair = tTestResultEachPair$Difference.Between.Means[which(row.names(tTestResultEachPair$P.Values) %in% sigProbes.pair), j]
  highDiffProbes.pair = names(tail(sort(sigDiff.pair),3))
  lowDiffProbes.pair = names(head(sort(sigDiff.pair),3))

  chosenProbes.pair = c(chosenProbes.pair, highDiffProbes.pair, lowDiffProbes.pair)
}

id.pair <- duplicated(chosenProbes.pair) | duplicated(chosenProbes.pair, fromLast = TRUE)
chosenProbes.pair.dup = chosenProbes.pair[!id.pair]

chosenProbes_all = union(chosenProbes.one.dup, chosenProbes.pair.dup)


```

#### onevsRest & each pair
```{r}
heatmap.2(as.matrix(rna_known[chosenProbes_all,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          colRow = 0.1,
          main = "heatmap.onevsRest and each pair")


```

```{r}
## separate green from yellow, Luminal A and Luminal B
chosenProbes.pair = c()
j=2
  sigProbes.pair = row.names(tTestResultEachPair$P.Values[tTestResultEachPair$P.Values[,j] < 1e-8,])
  sigDiff.pair = tTestResultEachPair$Difference.Between.Means[which(row.names(tTestResultEachPair$P.Values) %in% sigProbes.pair), j]
  highDiffProbes.pair = names(tail(sort(sigDiff.pair),3))
  lowDiffProbes.pair = names(head(sort(sigDiff.pair),3))
  
  chosenProbes.pair= c(chosenProbes.pair, highDiffProbes.pair, lowDiffProbes.pair)


id.pair.LuminalAB <- duplicated(chosenProbes.pair) | duplicated(chosenProbes.pair, fromLast = TRUE)
chosenProbes.pair.LumianlAB = chosenProbes.pair[!id.pair.LuminalAB]

chosenProbes = union(chosenProbes.one.dup, chosenProbes.pair.LumianlAB)
```

#### onevsRest & specific eachpair group

##### one group
```{r,echo=FALSE}
print("Heatmap onevsRest features plus the eachpair features of LuminalB.LuminalA")
```

```{r}
heatmap.2(as.matrix(rna_known[chosenProbes,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          cexRow = 0.5,
          main = "heatmap.sep")
```

```{r}
### separate blue from yellow, HER2-enriched and Luminal B
chosenProbes.pair = c()
j=1
  sigProbes.pair = row.names(tTestResultEachPair$P.Values[tTestResultEachPair$P.Values[,j] < 1e-8,])
  sigDiff.pair = tTestResultEachPair$Difference.Between.Means[which(row.names(tTestResultEachPair$P.Values) %in% sigProbes.pair), j]
  highDiffProbes.pair = names(tail(sort(sigDiff.pair),3))
  lowDiffProbes.pair = names(head(sort(sigDiff.pair),3))
 
  chosenProbes.pair= c(chosenProbes.pair, highDiffProbes.pair, lowDiffProbes.pair)


id.pair.HER2_LuminalB <- duplicated(chosenProbes.pair) | duplicated(chosenProbes.pair, fromLast = TRUE)
chosenProbes.pair.HER2_LuminalB = chosenProbes.pair[!id.pair.HER2_LuminalB]

chosenProbes = union(chosenProbes,chosenProbes.pair.HER2_LuminalB)
```

##### two groups
```{r,echo=FALSE}
print("Heatmap onevsRest features plus the eachpair features of LuminalB.LuminalA and the eachpair features of LuminalB.HER2-enriched")
```

```{r}
heatmap.2(as.matrix(rna_known[chosenProbes,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          cexRow = 0.4,
          main = "heatmap.sep")

```


```{r}
### separate green from purple, Luminal A and normal like
chosenProbes.pair = c()
j=9
  sigProbes.pair = row.names(tTestResultEachPair$P.Values[tTestResultEachPair$P.Values[,j] < 1e-8,])
  sigDiff.pair = tTestResultEachPair$Difference.Between.Means[which(row.names(tTestResultEachPair$P.Values) %in% sigProbes.pair), j]
  highDiffProbes.pair = names(tail(sort(sigDiff.pair),3))
  lowDiffProbes.pair = names(head(sort(sigDiff.pair),3))
  
  chosenProbes.pair= c(chosenProbes.pair, highDiffProbes.pair, lowDiffProbes.pair)


id.pair.normal_LuminalA <- duplicated(chosenProbes.pair) | duplicated(chosenProbes.pair, fromLast = TRUE)
chosenProbes.pair.normal_LumianlA = chosenProbes.pair[!id.pair.normal_LuminalA]

chosenProbes = union(chosenProbes,chosenProbes.pair.normal_LumianlA)
```

##### three groups
```{r,echo=FALSE}
print("Heatmap onevsRest features plus the eachpair features of LuminalB.LuminalA, the eachpair features of LuminalB.HER2-enriched and the eachpair features of LuminalA.Normal-like.")
```

```{r}
heatmap.2(as.matrix(rna_known[chosenProbes,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          cexRow = 0.3,
          main = "heatmap.sep")
```

#### Summary for feature selection
```{r,echo=FALSE}
print("We are going to chose the union of onevsRest features plus the eachpair features of LuminalB.LuminalA, the eachpair features of LuminalB.HER2-enriched and the eachpair features of LuminalA.Normal-like as my genes set for future classifiers.")

print("It has 87 genes in total. The heatmap is shown in the latest one above.")
print("The genes used as features are:")
print(chosenProbes)
```

#### Compare my features with PAM50 genes

##### PAM50 genes
```{r,echo=FALSE}
print("Heatmap of pam50 genes")
```

```{r}
heatmap.2(as.matrix(rna_known[pam50_genes[,1],]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          colRow = 0.5,
          main = "heatmap.sep")
```

##### my features
```{r,echo=FALSE}
print("Heatmap of my features")
```

```{r}
heatmap.2(as.matrix(rna_known[chosenProbes,]),
          trace="none",
          col = colorRampPalette(c('blue', 'yellow'))(12),
          ColSideColors=pam50.class.colors,
          labCol = FALSE,
          margins=c(3,10),
          colRow = 0.5,
          main = "heatmap.sep")
```

### K-fold cross validation
```{r}
x=t(rna_known[chosenProbes,])
colnames(x)=gsub("-",".",colnames(x))
y=pam50_class_micro[,1]

set.seed(111)
folds <- createFolds(factor(y), k = 10, list = TRUE)

x_pam=t(rna_known[as.character(pam50_genes[,1]),])

```

### SVM

#### svm_Linear

##### svm_Linear with my features
```{r}
acc_svm_linear=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))

  set.seed(9)
  svm_Linear=train(y~.,data=dat_train,method="svmLinear", preProcess = c("center", "scale"),
tuneLength = 10)
# print(svm_Linear)
  
  test_pred=predict(svm_Linear,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_linear=c(acc_svm_linear,table$overall["Accuracy"])
  
  
}
print(acc_svm_linear)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8888889 0.8888889 0.8593750 0.7903226 0.8064516 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8548387 0.8571429 0.8750000 0.8852459 0.8709677 

print(mean(acc_svm_linear))
  # 0.8577122
```

##### svm_Linear with pam50 genes
```{r}
acc_svm_linear_pam=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))

  set.seed(19)
  svm_Linear_pam=train(y~.,data=dat_train,method="svmLinear",preProcess = c("center", "scale"),
tuneLength = 10)
  
  test_pred=predict(svm_Linear_pam,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_linear_pam=c(acc_svm_linear_pam,table$overall["Accuracy"])

  #  print(i)
}
print(acc_svm_linear_pam)
#  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9682540 0.9047619 0.8593750 0.7903226 0.9032258 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8709677 0.9047619 0.8281250 0.8852459 0.9193548 

print(mean(acc_svm_linear_pam))
# 0.8834395
```


#### svm_polynomial

##### svm_polynomial with my features
```{r}
C <- c(0.1,1,10,100)
degree <- c(1,2,3)
scale <- 1
sigma <- c(0.0001,0.001,0.01,0.1,1)

gr.poly <- expand.grid(C=C,degree=degree,scale=scale)


acc_svm_poly=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
  
  set.seed(123)
  svm_poly=train(y~.,data=dat_train,method='svmPoly', preProcess = c("center", "scale"),tuneGrid=gr.poly)
#   print(svm_poly)
  test_pred=predict(svm_poly,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_poly=c(acc_svm_poly,table$overall["Accuracy"])
  
#print(i)
}
print(acc_svm_poly)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9206349 0.9047619 0.8281250 0.7741935 0.7903226 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8387097 0.8095238 0.8281250 0.8196721 0.8064516  

print(mean(acc_svm_poly))
# 0.832052
```

##### svm_polynomial with pam50 genes
```{r}
C <- c(0.1,1,10,100)
degree <- c(1,2,3)
scale <- 1
sigma <- c(0.0001,0.001,0.01,0.1,1)

gr.poly <- expand.grid(C=C,degree=degree,scale=scale)


acc_svm_poly_pam=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
  
  set.seed(231)
  svm_poly_pam=train(y~.,data=dat_train,method='svmPoly', preProcess = c("center", "scale"),tuneGrid=gr.poly)
 
  test_pred=predict(svm_poly_pam,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_poly_pam=c(acc_svm_poly_pam,table$overall["Accuracy"])
  
#  print(i)
}
print(acc_svm_poly_pam)
#   Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9682540 0.9047619 0.8906250 0.8064516 0.9193548 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8870968 0.9365079 0.9375000 0.9016393 0.8870968 
print(mean(acc_svm_poly_pam))
# 0.9039288
```

#### svm_radial

##### svm_radial with my features
```{r}
C <- c(0.1,1,10,100)
sigma <- c(0.0001,0.001,0.01,0.1,1)

gr.radial <- expand.grid(C=C,sigma=sigma)

acc_svm_radial=NULL

for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
  
  set.seed(234)
  svm_radial=train(y~.,data=dat_train,method='svmRadial',preProcess = c("center", "scale"),tuneGrid=gr.radial)
  
  test_pred=predict(svm_radial,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_radial=c(acc_svm_radial,table$overall["Accuracy"])
  
#  print(i)
}
print(acc_svm_radial)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9047619 0.8571429 0.8125000 0.8548387 0.8225806 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8064516 0.8412698 0.8750000 0.9016393 0.8387097 

print(mean(acc_svm_radial))
# 0.8514895
```

##### svm_radial with pam50 genes
```{r}
C <- c(0.1,1,10,100)
sigma <- c(0.0001,0.001,0.01,0.1,1)

gr.radial <- expand.grid(C=C,sigma=sigma)

acc_svm_radial_pam=NULL

for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
  
  set.seed(345)
  svm_radial_pam=train(y~.,data=dat_train,method='svmRadial',preProcess = c("center", "scale"),tuneGrid=gr.radial)
  
  test_pred=predict(svm_radial_pam,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_svm_radial_pam=c(acc_svm_radial_pam,table$overall["Accuracy"])
  
#  print(i)
}

print(acc_svm_radial_pam)
#Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9523810 0.8888889 0.8750000 0.8709677 0.9516129 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9032258 0.9206349 0.9375000 0.9016393 0.8548387 

print(mean(acc_svm_radial_pam))
#  0.9056689
```

#### summary of SVM
```{r,echo=FALSE}
print("The svm model with radial basis function kernel on the pam50 genes has achieved the highest accuracy for prediction on test averaged over 10 folds, which is 0.9056689.")
print("For now the best combination of classifier and gene set, is svm_radial and pam50 gene set. ")
```

### Random forest

#### Random forest with my features
```{r}
acc_rf=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
  
  set.seed(71)
  rf <- randomForest(y ~ .,data=dat_train)
  #print(rf)
  
  test_pred=predict(rf,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_rf=c(acc_rf,table$overall["Accuracy"])
#print(i)
}
print(acc_rf)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9047619 0.9206349 0.8281250 0.8548387 0.8709677 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8387097 0.8888889 0.8750000 0.9508197 0.8064516 
print(mean(acc_rf))
# 0.8739198
```

#### Random forest with pam50 genes
```{r}
acc_rf_pam=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  dat_train=data.frame(x_train,y=as.factor(y_train))
 
  set.seed(81)
  rf_pam <- randomForest(y ~ .,data=dat_train)
 # print(rf)
  
  test_pred=predict(rf_pam,newdata=x_test)
  table=confusionMatrix(test_pred,y_test)
  acc_rf_pam=c(acc_rf_pam,table$overall["Accuracy"])
#print(i)
}
print(acc_rf_pam)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9365079 0.9365079 0.8593750 0.9032258 0.9032258 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9032258 0.9047619 0.9062500 0.8688525 0.8870968 
print(mean(acc_rf_pam))
# 0.9009029
```

#### summary of Random Forest
```{r,echo=FALSE}
print("The random forest model trained on pam50 genes has achieved higher accuracy for prediction on test averaged over 10 folds, which is 0.9009029. But this is lower than svm_radial.")
print("For now the best combination of classifier and gene set, is still svm_radial and pam50 gene set. ")
```


### KNN

#### KNN with number of neighbours set to 3

##### k=3, my features
```{r}
acc_knn3=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(11)
  knn_3 <- knn(x_train,x_test,cl=y_train,k=3)
  
  table=confusionMatrix(knn_3,y_test)
  acc_knn3=c(acc_knn3,table$overall["Accuracy"])
#print(i)
}

print(acc_knn3)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8095238 0.8253968 0.7812500 0.7096774 0.8064516 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8225806 0.7619048 0.8281250 0.8852459 0.7903226 
print(mean(acc_knn3))
# 0.8020479
```


##### k=3, pam50 genes
```{r}
acc_knn3_pam=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(12)
  knn_3_pam <- knn(x_train,x_test,cl=y_train,k=3)
  
  table=confusionMatrix(knn_3_pam,y_test)
  acc_knn3_pam=c(acc_knn3_pam,table$overall["Accuracy"])
#print(i)
}

print(acc_knn3_pam)
#  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9682540 0.8412698 0.8281250 0.8548387 0.9032258 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8548387 0.8253968 0.8593750 0.9016393 0.8870968 

print(mean(acc_knn3_pam))
# 0.872406
```


#### KNN with number of neighbours set to 5

##### k=5, my features
```{r}
acc_knn5=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(21)
  knn_5 <- knn(x_train,x_test,cl=y_train,k=5)
  
  table=confusionMatrix(knn_5,y_test)
  acc_knn5=c(acc_knn5,table$overall["Accuracy"])
#print(i)
}

print(acc_knn5)
#  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.7777778 0.8571429 0.7968750 0.7580645 0.8225806 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8225806 0.8571429 0.9062500 0.8524590 0.7741935 
print(mean(acc_knn5))
# 0.8225067
```


##### k=5, pam50 genes
```{r}
acc_knn5_pam=NULL

for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(54)
  knn_5_pam <- knn(x_train,x_test,cl=y_train,k=5)
  
  table=confusionMatrix(knn_5_pam,y_test)
  acc_knn5_pam=c(acc_knn5_pam,table$overall["Accuracy"])
#print(i)
}

print(acc_knn5_pam)
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9523810 0.9047619 0.8281250 0.8870968 0.9032258 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9032258 0.8571429 0.8750000 0.9016393 0.8709677 

print(mean(acc_knn5_pam))
#0.8883566
```


#### KNN with number of neighbours set to 7

##### k=7, my features
```{r}
acc_knn7=NULL
for(i in 1:10){
  
  index=folds[[i]]
  x_train=x[-index,]
  x_test=x[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(7)
  knn_7 <- knn(x_train,x_test,cl=y_train,k=7)
  
  table=confusionMatrix(knn_7,y_test)
  acc_knn7=c(acc_knn7,table$overall["Accuracy"])
#print(i)
}

print(acc_knn7)
#  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8253968 0.8253968 0.7812500 0.7258065 0.8225806 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8548387 0.7936508 0.8437500 0.8032787 0.7741935 

print(mean(acc_knn7))
# 0.8050142
```


##### k=7, pam50 genes
```{r}
acc_knn7_pam=NULL

for(i in 1:10){
  
  index=folds[[i]]
  x_train=x_pam[-index,]
  x_test=x_pam[index,]
  
  y_train=as.factor(y[-index])
  y_test=as.factor(y[index])
  
  set.seed(44)
  knn_7_pam <- knn(x_train,x_test,cl=y_train,k=7)
  
  table=confusionMatrix(knn_7_pam,y_test)
  acc_knn7_pam=c(acc_knn7_pam,table$overall["Accuracy"])
#print(i)
}

print(acc_knn7_pam)
#  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.9682540 0.9047619 0.8437500 0.8870968 0.8709677 
# Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
#0.8870968 0.8571429 0.8437500 0.9180328 0.8870968 

print(mean(acc_knn7_pam))
# 0.886795
```

#### Summary of KNN
```{r,echo=FALSE}
print("The knn classifier with number of neighbours set to 5, also trained on pam50 gene set has achieved the highest accuracy at 0.8883566.")

print("For now, the best performance is still achieved by classifier svm_radial and pam50 geneset.")
```

### Best classifier for prediction
```{r}
## we will take svm-radial classifier and pam50 gene set together to do predictions on the unknown RNA-seq data

C <- c(0.1,1,10,100)
sigma <- c(0.0001,0.001,0.01,0.1,1)

gr.radial <- expand.grid(C=C,sigma=sigma)

dat_train=data.frame(x_pam,y=as.factor(y))
set.seed(529)
svm_radial_pam=train(y~.,data=dat_train,method='svmRadial',preProcess = c("center", "scale"),tuneGrid=gr.radial)

#print(svm_radial_pam)

rna_unknown_pam=t(rna_unknown[pam50_genes[,1],])
pred_unknown=predict(svm_radial_pam,newdata=rna_unknown_pam)

```

### Survival analysis
```{r}
sample_info=c(rownames(rna_unknown_pam),rownames(pam50_class_micro))
sample_class=c(as.character(pred_unknown),pam50_class_micro[,1])

rownames(survival_data)=gsub("-",".",rownames(survival_data))
sample_info_survival=sample_info[which(as.character(sample_info)%in%rownames(survival_data))]
sample_class_survial=sample_class[which(as.character(sample_info)%in%rownames(survival_data))]

data_class=as.data.frame(as.matrix(sample_class_survial,ncol=1))
rownames(data_class)=sample_info_survival
colnames(data_class)="class"

data=merge(survival_data,data_class,by=0)

data$status=NA
data$status[data$vital_status=="Dead"]=1
data$status[data$vital_status=="Alive"]=0

fit=survival::survfit(Surv(time_to_death_or_last_contact,status)~class,data=data)

{plot(fit, col = 1:10, lty = 1, lwd = 1.5,  
     xlab = "Time", ylab = "Survival probability", cex.lab = 1.2)

title("Survival Curves of 5 breast cancer subtypes")
legend("topright", c(paste("Basal-like n =", fit$n[1]),
                     paste("HER2-enriched n =",fit$n[2]),
                     paste("Luminal A n =", fit$n[3]),
                     paste("Luminal B n =", fit$n[4]),
                     paste("Normal-like n =", fit$n[5])),
       
       col = 1:10, lty = 1, lwd = 1.5, bty = "n", cex = 0.6, text.col = 1:10)      }
```

